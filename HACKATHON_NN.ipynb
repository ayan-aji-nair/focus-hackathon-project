{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HACKATHON_NN",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "VuxxZi0Kfg8U"
      },
      "source": [
        "import torch\r\n",
        "from torchvision import datasets, transforms\r\n",
        "import helper\r\n",
        "\r\n",
        "# Define a transform to normalize the data\r\n",
        "transform = transforms.Compose([transforms.ToTensor,\r\n",
        "                                transforms.Normalize([0.5], [0.5])])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4n7reSnCF7LK"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HR6SpqmUGHxC"
      },
      "source": [
        "# reads in the climate data\r\n",
        "data = pd.read_csv('hack-data - Sheet1.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jc5sAzJDB5ge"
      },
      "source": [
        "\r\n",
        "from torch.utils.data import Dataset, DataLoader\r\n",
        "\r\n",
        "# creates a dataset\r\n",
        "class MyDataset(Dataset):\r\n",
        "    def __init__(self, root, n_inp, transform=None):\r\n",
        "        self.df = pd.read_csv(root)\r\n",
        "        self.data = self.df.to_numpy()\r\n",
        "        self.x , self.y = (torch.from_numpy(self.data[:,:n_inp]),\r\n",
        "                           torch.from_numpy(self.data[:,n_inp:]))\r\n",
        "        self.x = self.x.type(torch.float32)\r\n",
        "        self.y = self.y.type(torch.float32)\r\n",
        "        self.transform=transform\r\n",
        "    def __getitem__(self, idx):\r\n",
        "        x = self.x[idx, :]\r\n",
        "        y = self.y[idx,:]\r\n",
        "\r\n",
        "        return x,y\r\n",
        "    def __len__(self):\r\n",
        "        return len(self.data)\r\n",
        "\r\n",
        "# creates dataset object\r\n",
        "myData = MyDataset(\"hack-data - Sheet1.csv\", 9, transform=transform)\r\n",
        "\r\n",
        "# creates data loader\r\n",
        "data_loader = DataLoader(myData, batch_size=4, shuffle =True)\r\n"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IkQQq4Ief0BP"
      },
      "source": [
        "from torch import nn, optim\r\n",
        "import torch.nn.functional as F\r\n",
        "\r\n",
        "n_in = 9 # climate \r\n",
        "n_out = 3\r\n",
        "\r\n",
        "# TODO: Define your network architecture here\r\n",
        "class Classifier(nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "        super().__init__()\r\n",
        "\r\n",
        "        # define the actual model\r\n",
        "        self.fc1 = nn.Linear(n_in, 32)\r\n",
        "        self.fc2 = nn.Linear(32, 16)\r\n",
        "        self.fc3 = nn.Linear(16, 8)\r\n",
        "        self.fc4 = nn.Linear(8, n_out)\r\n",
        "        \r\n",
        "    def forward(self, x):\r\n",
        "        # run an input data point through the model\r\n",
        "        x = x.view(x.shape[0], -1)\r\n",
        "        \r\n",
        "        x = F.relu(self.fc1(x))\r\n",
        "        x = F.relu(self.fc2(x))\r\n",
        "        x = F.relu(self.fc3(x))\r\n",
        "        x = self.fc4(x)\r\n",
        "        \r\n",
        "        return x"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JvUDVGWpf1TZ"
      },
      "source": [
        "# create the model, define the loss and the optimizer\r\n",
        "model = Classifier()\r\n",
        "criterion = nn.MSELoss()\r\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.003)"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tq-c_5-dgEYJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3685b36-11ba-4e0a-9c5f-a27e2e4fad2f"
      },
      "source": [
        "# create the training loop for the model to learn\r\n",
        "epochs = 500\r\n",
        "\r\n",
        "for e in range(epochs):\r\n",
        "    running_loss = 0\r\n",
        "    for data, labels in data_loader:\r\n",
        "        log_ps = model(data.float())\r\n",
        "        loss = criterion(log_ps.float(), labels)\r\n",
        "        \r\n",
        "        optimizer.zero_grad()\r\n",
        "        loss.backward()\r\n",
        "        optimizer.step()\r\n",
        "        \r\n",
        "        running_loss += loss.item()\r\n",
        "    else:\r\n",
        "        print(f\"Training loss: {running_loss/len(data_loader)}\")"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training loss: 23386.78025390625\n",
            "Training loss: 20566.840126953124\n",
            "Training loss: 19554.133466796877\n",
            "Training loss: 18731.6385546875\n",
            "Training loss: 19330.568657226562\n",
            "Training loss: 17680.21743652344\n",
            "Training loss: 17447.264853515626\n",
            "Training loss: 17430.716884765625\n",
            "Training loss: 17930.595161132813\n",
            "Training loss: 17629.9675390625\n",
            "Training loss: 15732.4459765625\n",
            "Training loss: 17437.715068359375\n",
            "Training loss: 15571.157705078125\n",
            "Training loss: 18404.282983398436\n",
            "Training loss: 15953.117143554688\n",
            "Training loss: 16169.9787109375\n",
            "Training loss: 16851.679321289062\n",
            "Training loss: 17004.385859375\n",
            "Training loss: 15671.531586914063\n",
            "Training loss: 17215.71923828125\n",
            "Training loss: 15493.48505859375\n",
            "Training loss: 16755.094501953125\n",
            "Training loss: 14687.8016015625\n",
            "Training loss: 16043.124580078125\n",
            "Training loss: 16091.5494140625\n",
            "Training loss: 14066.00439453125\n",
            "Training loss: 14858.73177734375\n",
            "Training loss: 14634.729013671875\n",
            "Training loss: 14388.631015625\n",
            "Training loss: 15159.214672851562\n",
            "Training loss: 17401.419453125\n",
            "Training loss: 14999.357275390625\n",
            "Training loss: 13842.671743164063\n",
            "Training loss: 14497.99166015625\n",
            "Training loss: 14575.594794921875\n",
            "Training loss: 14275.676889648437\n",
            "Training loss: 13624.185078125\n",
            "Training loss: 14321.465791015626\n",
            "Training loss: 14861.7795703125\n",
            "Training loss: 13935.732329101562\n",
            "Training loss: 14513.387329101562\n",
            "Training loss: 13377.780146484374\n",
            "Training loss: 13348.264272460938\n",
            "Training loss: 13280.186137695313\n",
            "Training loss: 13619.779663085938\n",
            "Training loss: 13143.027490234375\n",
            "Training loss: 13243.64078125\n",
            "Training loss: 13768.185439453126\n",
            "Training loss: 13385.4535546875\n",
            "Training loss: 12869.0625390625\n",
            "Training loss: 13637.16591796875\n",
            "Training loss: 13213.26728515625\n",
            "Training loss: 13000.285415039063\n",
            "Training loss: 13335.7946484375\n",
            "Training loss: 12382.0221875\n",
            "Training loss: 12707.46017578125\n",
            "Training loss: 13095.177338867188\n",
            "Training loss: 11987.719248046875\n",
            "Training loss: 12548.87224609375\n",
            "Training loss: 15573.353608398438\n",
            "Training loss: 13671.32560546875\n",
            "Training loss: 12141.789584960938\n",
            "Training loss: 12979.162416992187\n",
            "Training loss: 12173.252465820313\n",
            "Training loss: 12569.456723632813\n",
            "Training loss: 13286.65814453125\n",
            "Training loss: 13872.534301757812\n",
            "Training loss: 12350.82791015625\n",
            "Training loss: 12803.312734375\n",
            "Training loss: 12506.446767578125\n",
            "Training loss: 13147.408603515625\n",
            "Training loss: 12466.656494140625\n",
            "Training loss: 12392.380322265624\n",
            "Training loss: 12044.112309570313\n",
            "Training loss: 11989.570517578126\n",
            "Training loss: 12535.034096679687\n",
            "Training loss: 13359.28734375\n",
            "Training loss: 13245.87787109375\n",
            "Training loss: 12370.957099609375\n",
            "Training loss: 12293.2728515625\n",
            "Training loss: 12033.276982421876\n",
            "Training loss: 11543.603393554688\n",
            "Training loss: 12395.447880859376\n",
            "Training loss: 11714.409296875\n",
            "Training loss: 14315.06896484375\n",
            "Training loss: 12683.712705078126\n",
            "Training loss: 12909.4031640625\n",
            "Training loss: 11845.180625\n",
            "Training loss: 12250.8683984375\n",
            "Training loss: 11676.016606445313\n",
            "Training loss: 11503.162758789062\n",
            "Training loss: 12027.093178710937\n",
            "Training loss: 12044.916127929688\n",
            "Training loss: 12041.03146484375\n",
            "Training loss: 12587.746025390625\n",
            "Training loss: 11986.376435546876\n",
            "Training loss: 11351.196801757813\n",
            "Training loss: 11499.45705078125\n",
            "Training loss: 11616.25373046875\n",
            "Training loss: 12475.258081054688\n",
            "Training loss: 13096.585380859375\n",
            "Training loss: 13386.43072265625\n",
            "Training loss: 11730.94279296875\n",
            "Training loss: 11568.561552734374\n",
            "Training loss: 12403.966801757813\n",
            "Training loss: 12549.36904296875\n",
            "Training loss: 12334.09544921875\n",
            "Training loss: 11744.189995117187\n",
            "Training loss: 12293.925888671874\n",
            "Training loss: 11531.837001953125\n",
            "Training loss: 12394.326381835937\n",
            "Training loss: 11719.887866210938\n",
            "Training loss: 11420.092583007812\n",
            "Training loss: 11811.496943359374\n",
            "Training loss: 11418.630751953126\n",
            "Training loss: 11651.502822265626\n",
            "Training loss: 12058.486401367187\n",
            "Training loss: 12057.5333203125\n",
            "Training loss: 12235.158291015625\n",
            "Training loss: 11609.425244140624\n",
            "Training loss: 11770.717041015625\n",
            "Training loss: 11874.464912109375\n",
            "Training loss: 11558.7687890625\n",
            "Training loss: 11303.80205078125\n",
            "Training loss: 12368.25220703125\n",
            "Training loss: 12230.748876953125\n",
            "Training loss: 11094.304404296876\n",
            "Training loss: 12423.90783203125\n",
            "Training loss: 11708.153500976563\n",
            "Training loss: 11362.379697265626\n",
            "Training loss: 12190.290092773437\n",
            "Training loss: 12867.240620117187\n",
            "Training loss: 11807.429150390624\n",
            "Training loss: 11792.131943359374\n",
            "Training loss: 11208.2423046875\n",
            "Training loss: 12641.626416015624\n",
            "Training loss: 12637.003793945312\n",
            "Training loss: 11716.632973632812\n",
            "Training loss: 12854.954848632813\n",
            "Training loss: 12323.494577636719\n",
            "Training loss: 11663.983969726563\n",
            "Training loss: 11232.925595703126\n",
            "Training loss: 11565.987465820312\n",
            "Training loss: 11236.398359375\n",
            "Training loss: 11187.101005859375\n",
            "Training loss: 11777.905708007813\n",
            "Training loss: 11153.171552734375\n",
            "Training loss: 11231.388076171876\n",
            "Training loss: 12503.875483398437\n",
            "Training loss: 11137.003442382813\n",
            "Training loss: 11294.026196289062\n",
            "Training loss: 11497.680053710938\n",
            "Training loss: 11393.576245117187\n",
            "Training loss: 11153.098334960938\n",
            "Training loss: 11304.955874023437\n",
            "Training loss: 11721.001020507812\n",
            "Training loss: 11207.184091796875\n",
            "Training loss: 12132.940766601563\n",
            "Training loss: 12546.97298828125\n",
            "Training loss: 11821.9754296875\n",
            "Training loss: 11364.587578125\n",
            "Training loss: 11584.07220703125\n",
            "Training loss: 11523.31853515625\n",
            "Training loss: 12287.588598632812\n",
            "Training loss: 11402.887495117187\n",
            "Training loss: 11535.125434570313\n",
            "Training loss: 11713.1539453125\n",
            "Training loss: 11508.39328125\n",
            "Training loss: 11565.172666015626\n",
            "Training loss: 12724.449228515625\n",
            "Training loss: 11837.872119140626\n",
            "Training loss: 11087.388291015624\n",
            "Training loss: 11303.991640625\n",
            "Training loss: 12357.801469726563\n",
            "Training loss: 11642.142817382812\n",
            "Training loss: 11792.090927734374\n",
            "Training loss: 11130.749291992188\n",
            "Training loss: 11035.775844726562\n",
            "Training loss: 12884.48365234375\n",
            "Training loss: 11387.90734375\n",
            "Training loss: 11379.232919921875\n",
            "Training loss: 11589.391181640625\n",
            "Training loss: 11186.049985351563\n",
            "Training loss: 12731.918759765626\n",
            "Training loss: 11229.572534179688\n",
            "Training loss: 11538.411411132813\n",
            "Training loss: 11221.51935546875\n",
            "Training loss: 11504.664672851562\n",
            "Training loss: 11586.19896484375\n",
            "Training loss: 10971.803701171875\n",
            "Training loss: 11204.8250390625\n",
            "Training loss: 11810.076796875\n",
            "Training loss: 10798.319091796875\n",
            "Training loss: 11199.414111328126\n",
            "Training loss: 10941.66466796875\n",
            "Training loss: 11409.086323242187\n",
            "Training loss: 11264.009184570312\n",
            "Training loss: 11107.89625\n",
            "Training loss: 11125.6399609375\n",
            "Training loss: 11539.602998046876\n",
            "Training loss: 11195.08068359375\n",
            "Training loss: 11159.509228515624\n",
            "Training loss: 11054.516220703124\n",
            "Training loss: 11550.461982421875\n",
            "Training loss: 11301.333530273438\n",
            "Training loss: 11406.189296875\n",
            "Training loss: 10965.54765625\n",
            "Training loss: 11437.633139648438\n",
            "Training loss: 11133.600180664063\n",
            "Training loss: 11282.068051757813\n",
            "Training loss: 11055.608549804687\n",
            "Training loss: 11181.0526953125\n",
            "Training loss: 11524.359838867187\n",
            "Training loss: 11746.45703125\n",
            "Training loss: 11128.26751953125\n",
            "Training loss: 11445.181904296875\n",
            "Training loss: 11461.327358398437\n",
            "Training loss: 11047.295200195313\n",
            "Training loss: 10857.790854492188\n",
            "Training loss: 10860.992568359376\n",
            "Training loss: 10867.564853515625\n",
            "Training loss: 10968.657612304687\n",
            "Training loss: 12113.448491210938\n",
            "Training loss: 10938.025668945313\n",
            "Training loss: 10979.09048828125\n",
            "Training loss: 10576.2305859375\n",
            "Training loss: 11757.062143554687\n",
            "Training loss: 10743.20919921875\n",
            "Training loss: 10858.517680664063\n",
            "Training loss: 10903.753022460938\n",
            "Training loss: 10768.113701171875\n",
            "Training loss: 10727.33033203125\n",
            "Training loss: 10774.1009375\n",
            "Training loss: 10735.131689453125\n",
            "Training loss: 10837.3009375\n",
            "Training loss: 10823.785576171875\n",
            "Training loss: 10929.35548828125\n",
            "Training loss: 10905.70001953125\n",
            "Training loss: 10895.699780273437\n",
            "Training loss: 10625.526166992187\n",
            "Training loss: 10970.75970703125\n",
            "Training loss: 11067.582646484376\n",
            "Training loss: 10814.308422851562\n",
            "Training loss: 11357.92572265625\n",
            "Training loss: 10674.800830078126\n",
            "Training loss: 10874.504638671875\n",
            "Training loss: 11706.501123046875\n",
            "Training loss: 11786.134877929688\n",
            "Training loss: 11015.898583984375\n",
            "Training loss: 10733.069951171876\n",
            "Training loss: 11187.021147460937\n",
            "Training loss: 10641.873525390625\n",
            "Training loss: 11086.90712890625\n",
            "Training loss: 11159.2279296875\n",
            "Training loss: 11515.472060546876\n",
            "Training loss: 11411.679365234375\n",
            "Training loss: 10543.21404296875\n",
            "Training loss: 11166.858676757813\n",
            "Training loss: 10758.562446289063\n",
            "Training loss: 11223.842568359374\n",
            "Training loss: 10992.4296875\n",
            "Training loss: 11367.097314453125\n",
            "Training loss: 10508.017192382813\n",
            "Training loss: 10223.620180664062\n",
            "Training loss: 11301.409697265624\n",
            "Training loss: 10861.923642578126\n",
            "Training loss: 10718.817841796876\n",
            "Training loss: 10598.3650390625\n",
            "Training loss: 10615.033779296875\n",
            "Training loss: 11172.551923828125\n",
            "Training loss: 11065.212124023437\n",
            "Training loss: 11156.09462890625\n",
            "Training loss: 10395.822744140625\n",
            "Training loss: 10557.147236328125\n",
            "Training loss: 10901.047763671875\n",
            "Training loss: 10489.749453125\n",
            "Training loss: 11166.872529296876\n",
            "Training loss: 11087.551025390625\n",
            "Training loss: 10517.2759375\n",
            "Training loss: 10827.586391601562\n",
            "Training loss: 11556.021318359375\n",
            "Training loss: 11101.425537109375\n",
            "Training loss: 11858.91884765625\n",
            "Training loss: 10881.46779296875\n",
            "Training loss: 11066.055419921875\n",
            "Training loss: 10510.492016601562\n",
            "Training loss: 10366.289873046875\n",
            "Training loss: 11090.32677734375\n",
            "Training loss: 11066.222138671876\n",
            "Training loss: 10562.398100585937\n",
            "Training loss: 10661.169204101563\n",
            "Training loss: 10965.801875\n",
            "Training loss: 10862.802563476562\n",
            "Training loss: 11048.394038085937\n",
            "Training loss: 11526.003393554687\n",
            "Training loss: 10976.893120117187\n",
            "Training loss: 10732.811538085938\n",
            "Training loss: 10623.833017578125\n",
            "Training loss: 11567.607568359375\n",
            "Training loss: 11094.669375\n",
            "Training loss: 10387.258662109374\n",
            "Training loss: 10266.309384765626\n",
            "Training loss: 10249.7463671875\n",
            "Training loss: 10604.692099609376\n",
            "Training loss: 10923.327456054687\n",
            "Training loss: 10654.19650390625\n",
            "Training loss: 10347.864970703125\n",
            "Training loss: 10978.265229492188\n",
            "Training loss: 11255.52021484375\n",
            "Training loss: 11335.261669921874\n",
            "Training loss: 10643.68748046875\n",
            "Training loss: 10232.533461914063\n",
            "Training loss: 11088.201884765625\n",
            "Training loss: 11785.2254296875\n",
            "Training loss: 10277.907377929687\n",
            "Training loss: 10278.749819335937\n",
            "Training loss: 10692.34310546875\n",
            "Training loss: 11145.321538085938\n",
            "Training loss: 11558.1015234375\n",
            "Training loss: 10456.236123046874\n",
            "Training loss: 10673.1143359375\n",
            "Training loss: 10630.38150390625\n",
            "Training loss: 10557.729575195313\n",
            "Training loss: 10622.799638671875\n",
            "Training loss: 10985.129697265626\n",
            "Training loss: 10516.202138671875\n",
            "Training loss: 10451.283920898437\n",
            "Training loss: 10563.961240234376\n",
            "Training loss: 11031.897314453125\n",
            "Training loss: 10626.556796875\n",
            "Training loss: 12152.189189453125\n",
            "Training loss: 11400.227490234374\n",
            "Training loss: 11197.67984375\n",
            "Training loss: 10803.30830078125\n",
            "Training loss: 10815.951069335937\n",
            "Training loss: 11138.732573242187\n",
            "Training loss: 10944.98373046875\n",
            "Training loss: 10300.215673828125\n",
            "Training loss: 10530.789501953124\n",
            "Training loss: 10611.556118164062\n",
            "Training loss: 10745.420981445313\n",
            "Training loss: 10519.5461328125\n",
            "Training loss: 10445.2902734375\n",
            "Training loss: 10358.58806640625\n",
            "Training loss: 10750.2929296875\n",
            "Training loss: 10775.291083984375\n",
            "Training loss: 10364.796513671876\n",
            "Training loss: 10421.994643554688\n",
            "Training loss: 10317.824399414063\n",
            "Training loss: 11415.029155273438\n",
            "Training loss: 11002.589213867188\n",
            "Training loss: 11627.595283203125\n",
            "Training loss: 10289.1011328125\n",
            "Training loss: 10611.487880859375\n",
            "Training loss: 10799.022514648437\n",
            "Training loss: 11219.516997070312\n",
            "Training loss: 10513.809526367188\n",
            "Training loss: 11118.50775390625\n",
            "Training loss: 11169.446098632812\n",
            "Training loss: 10629.87623046875\n",
            "Training loss: 11029.974877929688\n",
            "Training loss: 10233.287622070313\n",
            "Training loss: 11063.657963867188\n",
            "Training loss: 10679.091513671876\n",
            "Training loss: 10272.792685546876\n",
            "Training loss: 10170.72861328125\n",
            "Training loss: 10221.78251953125\n",
            "Training loss: 11374.107646484375\n",
            "Training loss: 11403.641391601563\n",
            "Training loss: 10470.22375\n",
            "Training loss: 10806.357138671876\n",
            "Training loss: 10646.898828125\n",
            "Training loss: 10294.31376953125\n",
            "Training loss: 10070.198452148437\n",
            "Training loss: 10973.878427734375\n",
            "Training loss: 10919.181284179687\n",
            "Training loss: 11174.20890625\n",
            "Training loss: 10514.885278320313\n",
            "Training loss: 10564.189736328124\n",
            "Training loss: 12514.670380859376\n",
            "Training loss: 11213.510297851562\n",
            "Training loss: 10960.232553710937\n",
            "Training loss: 10408.143754882813\n",
            "Training loss: 10256.574946289062\n",
            "Training loss: 10323.805708007812\n",
            "Training loss: 10280.407016601563\n",
            "Training loss: 10510.15046875\n",
            "Training loss: 10025.460478515624\n",
            "Training loss: 10558.255434570312\n",
            "Training loss: 10095.62912109375\n",
            "Training loss: 10346.7508984375\n",
            "Training loss: 10789.909936523438\n",
            "Training loss: 10767.239106445313\n",
            "Training loss: 10200.217197265625\n",
            "Training loss: 10600.14361328125\n",
            "Training loss: 10253.4344140625\n",
            "Training loss: 10233.678291015625\n",
            "Training loss: 10102.246254882812\n",
            "Training loss: 10252.532006835938\n",
            "Training loss: 10533.356591796875\n",
            "Training loss: 10367.448828125\n",
            "Training loss: 10503.318061523438\n",
            "Training loss: 10312.575307617188\n",
            "Training loss: 10537.176171875\n",
            "Training loss: 10458.002001953126\n",
            "Training loss: 10492.79400390625\n",
            "Training loss: 10991.841474609375\n",
            "Training loss: 10136.82796875\n",
            "Training loss: 10466.07298828125\n",
            "Training loss: 10974.955830078125\n",
            "Training loss: 10283.668974609374\n",
            "Training loss: 10293.922900390626\n",
            "Training loss: 10614.801694335938\n",
            "Training loss: 10332.401005859376\n",
            "Training loss: 10309.92119140625\n",
            "Training loss: 10283.182983398438\n",
            "Training loss: 10324.622783203125\n",
            "Training loss: 10117.583139648437\n",
            "Training loss: 10273.068361816406\n",
            "Training loss: 10438.19052734375\n",
            "Training loss: 10110.99083984375\n",
            "Training loss: 9933.014770507812\n",
            "Training loss: 10120.949501953124\n",
            "Training loss: 10099.871259765625\n",
            "Training loss: 10223.499096679687\n",
            "Training loss: 10265.582016601562\n",
            "Training loss: 10329.439907226562\n",
            "Training loss: 10185.7911328125\n",
            "Training loss: 9835.56857421875\n",
            "Training loss: 10479.520107421875\n",
            "Training loss: 10273.9269921875\n",
            "Training loss: 10318.257885742187\n",
            "Training loss: 10405.559057617187\n",
            "Training loss: 10670.205249023438\n",
            "Training loss: 10343.980991210938\n",
            "Training loss: 10883.282202148437\n",
            "Training loss: 11249.3656640625\n",
            "Training loss: 10213.06232421875\n",
            "Training loss: 10769.292587890624\n",
            "Training loss: 10422.454458007813\n",
            "Training loss: 9981.88646484375\n",
            "Training loss: 10718.706303710938\n",
            "Training loss: 10428.218461914063\n",
            "Training loss: 10430.933203125\n",
            "Training loss: 10147.254033203126\n",
            "Training loss: 10571.145639648437\n",
            "Training loss: 10443.745439453125\n",
            "Training loss: 10810.12498046875\n",
            "Training loss: 10263.0937890625\n",
            "Training loss: 10209.64916015625\n",
            "Training loss: 10196.92248046875\n",
            "Training loss: 10288.835571289062\n",
            "Training loss: 10198.348388671875\n",
            "Training loss: 10181.087685546874\n",
            "Training loss: 10068.907319335938\n",
            "Training loss: 10178.923017578125\n",
            "Training loss: 10804.025336914063\n",
            "Training loss: 10341.958603515624\n",
            "Training loss: 10146.715815429687\n",
            "Training loss: 10237.30462890625\n",
            "Training loss: 10653.053364257812\n",
            "Training loss: 10253.096245117187\n",
            "Training loss: 10409.072646484376\n",
            "Training loss: 10636.089594726562\n",
            "Training loss: 10901.925756835937\n",
            "Training loss: 10181.945556640625\n",
            "Training loss: 10008.100673828125\n",
            "Training loss: 10106.073891601562\n",
            "Training loss: 10422.835205078125\n",
            "Training loss: 10453.80708984375\n",
            "Training loss: 10364.977099609376\n",
            "Training loss: 10119.23697265625\n",
            "Training loss: 10173.978725585937\n",
            "Training loss: 11253.63517578125\n",
            "Training loss: 11232.74240234375\n",
            "Training loss: 11212.274404296875\n",
            "Training loss: 10863.4433203125\n",
            "Training loss: 10281.06923828125\n",
            "Training loss: 10320.093681640625\n",
            "Training loss: 10227.998916015626\n",
            "Training loss: 10079.0471484375\n",
            "Training loss: 10190.135107421875\n",
            "Training loss: 10032.028774414062\n",
            "Training loss: 10172.907983398438\n",
            "Training loss: 10711.430180664063\n",
            "Training loss: 10585.729067382812\n",
            "Training loss: 10127.88751953125\n",
            "Training loss: 10272.22095703125\n",
            "Training loss: 10015.007475585937\n",
            "Training loss: 10308.3396875\n",
            "Training loss: 10274.967783203125\n",
            "Training loss: 10239.887199707031\n",
            "Training loss: 9848.93373046875\n",
            "Training loss: 10150.743452148437\n",
            "Training loss: 10401.7246484375\n",
            "Training loss: 9886.818256835937\n",
            "Training loss: 10380.37919921875\n",
            "Training loss: 10185.156557617187\n",
            "Training loss: 11243.081875\n",
            "Training loss: 10211.168359375\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_F8Z7Kh8-4G"
      },
      "source": [
        "import numpy as np\r\n",
        "\r\n",
        "# example input:\r\n",
        "# takes in an array of format [Temperature, Rainfall, Humidity (%),\tWind Speed,\tCrop Yield,\t\r\n",
        "#                              Acreage,\tNitrogen Soil Presence,\tPotassium Soil Presence,\t\r\n",
        "#                              Phosphorus Soil Presence]\r\n",
        "\r\n",
        "# create the example input\r\n",
        "sample_data = np.array([67,\t12,\t0.7,\t12,\t32000,\t100,\t0.35,\t0.75,\t0.32])\r\n",
        "sample_data = sample_data.reshape((9,1)).T\r\n",
        "\r\n",
        "# preprocessing\r\n",
        "sample_data = torch.from_numpy(sample_data).type(torch.float32)\r\n",
        "\r\n",
        "# get the output from the model\r\n",
        "out = model(sample_data)"
      ],
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "suMCmQRjQNsH",
        "outputId": "312ef5db-7318-49aa-a952-2d90de9b2495"
      },
      "source": [
        "# print the output\r\n",
        "print(out)"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[138.4817, 209.5386, 208.7975]], grad_fn=<AddmmBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJOh6QobQ2cJ"
      },
      "source": [
        "# save the model parameters\r\n",
        "torch.save(model.state_dict(), 'model_params')"
      ],
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2RjBrI_SIUg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}